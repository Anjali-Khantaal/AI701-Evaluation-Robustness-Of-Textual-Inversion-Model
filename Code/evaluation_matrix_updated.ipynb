{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FID SCORE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current working directory: /home/ishita.agarwal/Documents/ai_project_evaluation\n",
      "\n",
      "Full paths:\n",
      "Real images: /home/ishita.agarwal/Documents/ai_project_evaluation/mbzuai_paper_cup_Images\n",
      "Generated images: /home/ishita.agarwal/Documents/ai_project_evaluation/output\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/ishita.agarwal/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 85.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 real images:\n",
      "  - mbzuai_paper_cup_1.jpeg\n",
      "  - mbzuai_paper_cup_5.jpeg\n",
      "  - mbzuai_paper_cup_4.jpeg\n",
      "  - mbzuai_paper_cup_2.jpeg\n",
      "  - mbzuai_paper_cup_3.jpeg\n",
      "\n",
      "Found 9 generated images:\n",
      "  - generated_image_8.png\n",
      "  - generated_image_10.png\n",
      "  - generated_image_6.png\n",
      "  - generated_image_7.png\n",
      "  - generated_image_9.png\n",
      "  - generated_image_3.png\n",
      "  - generated_image_4.png\n",
      "  - generated_image_5.png\n",
      "  - generated_image_2.png\n",
      "\n",
      "Extracting features from real images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from generated images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 143.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature shapes:\n",
      "Real features: (5, 2048)\n",
      "Generated features: (9, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "FID Score: 164.8489\n",
      "FID Reliability: 10.00%\n",
      "\n",
      "Distance Statistics:\n",
      "Average Distance: 12.5460\n",
      "Min Distance: 10.1288\n",
      "Max Distance: 14.5523\n",
      "Standard Deviation: 0.4566\n"
     ]
    }
   ],
   "source": [
    "# small_dataset_evaluator.py\n",
    "\n",
    "import torch\n",
    "from pytorch_fid import fid_score\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def check_paths_and_files(real_path, generated_path):\n",
    "    \"\"\"Validate paths and check for image files.\"\"\"\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(real_path):\n",
    "        raise ValueError(f\"Real images directory not found: {real_path}\")\n",
    "    if not os.path.exists(generated_path):\n",
    "        raise ValueError(f\"Generated images directory not found: {generated_path}\")\n",
    "    \n",
    "    # List valid image extensions\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    \n",
    "    # Check real images\n",
    "    real_images = [f for f in os.listdir(real_path) \n",
    "                  if os.path.splitext(f.lower())[1] in valid_extensions]\n",
    "    if not real_images:\n",
    "        raise ValueError(f\"No valid images found in real directory: {real_path}\")\n",
    "    \n",
    "    # Check generated images\n",
    "    gen_images = [f for f in os.listdir(generated_path) \n",
    "                 if os.path.splitext(f.lower())[1] in valid_extensions]\n",
    "    if not gen_images:\n",
    "        raise ValueError(f\"No valid images found in generated directory: {generated_path}\")\n",
    "    \n",
    "    print(f\"\\nFound {len(real_images)} real images:\")\n",
    "    for img in real_images:\n",
    "        print(f\"  - {img}\")\n",
    "    \n",
    "    print(f\"\\nFound {len(gen_images)} generated images:\")\n",
    "    for img in gen_images:\n",
    "        print(f\"  - {img}\")\n",
    "    \n",
    "    return real_images, gen_images\n",
    "\n",
    "class SmallDatasetEvaluator:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize model with newer syntax to avoid warnings\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        self.model = models.resnet50(weights=weights)\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get the preprocessing from the weights\n",
    "        self.transform = weights.transforms()\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extract features from an image using ResNet.\"\"\"\n",
    "        try:\n",
    "            # Full path handling\n",
    "            full_path = str(Path(image_path).absolute())\n",
    "            if not os.path.exists(full_path):\n",
    "                print(f\"Image not found: {full_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Load and verify image\n",
    "            with Image.open(full_path) as img:\n",
    "                try:\n",
    "                    img.verify()  # Verify image is valid\n",
    "                except Exception as e:\n",
    "                    print(f\"Invalid image {full_path}: {e}\")\n",
    "                    return None\n",
    "            \n",
    "            # Reload image after verify\n",
    "            image = Image.open(full_path).convert('RGB')\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                features = self.model(image_tensor)\n",
    "            return features.squeeze().cpu().numpy().reshape(1, -1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_small_dataset(self, real_path, generated_path):\n",
    "        \"\"\"Comprehensive evaluation for small datasets.\"\"\"\n",
    "        # First check paths and files\n",
    "        real_images, gen_images = check_paths_and_files(real_path, generated_path)\n",
    "        \n",
    "        # Create full paths\n",
    "        real_paths = [os.path.join(real_path, img) for img in real_images]\n",
    "        gen_paths = [os.path.join(generated_path, img) for img in gen_images]\n",
    "        \n",
    "        # Extract features\n",
    "        real_features = []\n",
    "        print(\"\\nExtracting features from real images...\")\n",
    "        for path in tqdm(real_paths):\n",
    "            features = self.extract_features(path)\n",
    "            if features is not None:\n",
    "                real_features.append(features)\n",
    "        \n",
    "        gen_features = []\n",
    "        print(\"\\nExtracting features from generated images...\")\n",
    "        for path in tqdm(gen_paths):\n",
    "            features = self.extract_features(path)\n",
    "            if features is not None:\n",
    "                gen_features.append(features)\n",
    "        \n",
    "        if not real_features or not gen_features:\n",
    "            raise ValueError(\"Could not extract valid features from images\")\n",
    "        \n",
    "        # Stack features\n",
    "        real_features = np.vstack(real_features)\n",
    "        gen_features = np.vstack(gen_features)\n",
    "        \n",
    "        print(f\"\\nFeature shapes:\")\n",
    "        print(f\"Real features: {real_features.shape}\")\n",
    "        print(f\"Generated features: {gen_features.shape}\")\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = cdist(real_features, gen_features, 'euclidean')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = {\n",
    "            'nearest_neighbor_distance': np.min(distances, axis=0),\n",
    "            'average_distance': np.mean(distances, axis=0),\n",
    "            'worst_distance': np.max(distances, axis=0),\n",
    "            'distance_std': np.std(distances, axis=0),\n",
    "            'num_real': len(real_paths),\n",
    "            'num_generated': len(gen_paths)\n",
    "        }\n",
    "        \n",
    "        # Try to calculate FID\n",
    "        try:\n",
    "            fid_value = fid_score.calculate_fid_given_paths(\n",
    "                [real_path, generated_path],\n",
    "                batch_size=min(50, len(real_paths), len(gen_paths)),\n",
    "                device=self.device,\n",
    "                dims=2048\n",
    "            )\n",
    "            results['fid_score'] = fid_value\n",
    "            results['fid_reliability'] = min(1.0, (min(len(real_paths), len(gen_paths)) / 50))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not calculate FID score: {e}\")\n",
    "        \n",
    "        return results, distances\n",
    "\n",
    "def main():\n",
    "    # Get current working directory\n",
    "    cwd = os.getcwd()\n",
    "    print(f\"\\nCurrent working directory: {cwd}\")\n",
    "    \n",
    "    # Define paths\n",
    "    REAL_PATH = 'mbzuai_paper_cup_Images'\n",
    "    GENERATED_PATH = 'output'\n",
    "    \n",
    "    # Print full paths\n",
    "    real_full_path = os.path.abspath(REAL_PATH)\n",
    "    gen_full_path = os.path.abspath(GENERATED_PATH)\n",
    "    print(f\"\\nFull paths:\")\n",
    "    print(f\"Real images: {real_full_path}\")\n",
    "    print(f\"Generated images: {gen_full_path}\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = SmallDatasetEvaluator()\n",
    "        results, distances = evaluator.evaluate_small_dataset(REAL_PATH, GENERATED_PATH)\n",
    "        \n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        if 'fid_score' in results:\n",
    "            print(f\"FID Score: {results['fid_score']:.4f}\")\n",
    "            print(f\"FID Reliability: {results['fid_reliability']:.2%}\")\n",
    "        \n",
    "        print(\"\\nDistance Statistics:\")\n",
    "        print(f\"Average Distance: {np.mean(results['average_distance']):.4f}\")\n",
    "        print(f\"Min Distance: {np.min(results['nearest_neighbor_distance']):.4f}\")\n",
    "        print(f\"Max Distance: {np.max(results['worst_distance']):.4f}\")\n",
    "        print(f\"Standard Deviation: {np.mean(results['distance_std']):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during evaluation: {e}\")\n",
    "        \n",
    "        # Print more debug information\n",
    "        print(\"\\nDebug Information:\")\n",
    "        print(\"Checking directory contents:\")\n",
    "        for path, name in [(REAL_PATH, \"Real\"), (GENERATED_PATH, \"Generated\")]:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"\\n{name} directory contents:\")\n",
    "                for item in os.listdir(path):\n",
    "                    print(f\"  - {item}\")\n",
    "            else:\n",
    "                print(f\"\\n{name} directory not found!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishita.agarwal/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CLIP model...\n",
      "\n",
      "Evaluating images in output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████| 9/9 [00:05<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed successfully!\n",
      "Results saved in evaluation_results/\n"
     ]
    }
   ],
   "source": [
    "# mbzuai_clip_evaluator.py\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class MBZUAICLIPEvaluator:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load CLIP model and processor\n",
    "        print(\"Loading CLIP model...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Define MBZUAI-specific concepts and prompts\n",
    "        self.concepts = {\n",
    "            'core_elements': [\n",
    "                \"white paper cup\",\n",
    "                \"MBZUAI logo\",\n",
    "                \"clear logo visibility\",\n",
    "                \"proper logo placement\"\n",
    "            ],\n",
    "            'quality_aspects': [\n",
    "                \"high quality image\",\n",
    "                \"clear background\",\n",
    "                \"good lighting\",\n",
    "                \"proper contrast\"\n",
    "            ],\n",
    "            'design_elements': [\n",
    "                \"professional design\",\n",
    "                \"clean appearance\",\n",
    "                \"proper proportions\",\n",
    "                \"realistic cup shape\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Define specific prompts for evaluation\n",
    "        self.evaluation_prompts = [\n",
    "            \"a white paper cup with clear MBZUAI logo\",\n",
    "            \"professional paper cup with MBZUAI branding\",\n",
    "            \"high quality MBZUAI branded paper cup\",\n",
    "            \"clean white cup with MBZUAI logo\"\n",
    "        ]\n",
    "\n",
    "    def calculate_clip_scores(self, image, prompts):\n",
    "        \"\"\"Calculate CLIP scores for an image against multiple prompts.\"\"\"\n",
    "        try:\n",
    "            # Process image\n",
    "            image_inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            image_features = self.clip_model.get_image_features(**image_inputs)\n",
    "            \n",
    "            # Calculate scores for each prompt\n",
    "            scores = {}\n",
    "            for prompt in prompts:\n",
    "                text_inputs = self.clip_processor(text=[prompt], return_tensors=\"pt\", padding=True).to(self.device)\n",
    "                text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "                \n",
    "                similarity = torch.cosine_similarity(text_features, image_features).item()\n",
    "                scores[prompt] = similarity\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating scores: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_image(self, image_path):\n",
    "        \"\"\"Comprehensive evaluation of a single image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            results = {\n",
    "                'filename': os.path.basename(image_path),\n",
    "                'scores': {}\n",
    "            }\n",
    "            \n",
    "            # Evaluate against all concept categories\n",
    "            for category, concepts in self.concepts.items():\n",
    "                category_scores = self.calculate_clip_scores(image, concepts)\n",
    "                if category_scores:\n",
    "                    results['scores'][category] = category_scores\n",
    "            \n",
    "            # Evaluate against standard prompts\n",
    "            prompt_scores = self.calculate_clip_scores(image, self.evaluation_prompts)\n",
    "            if prompt_scores:\n",
    "                results['scores']['prompts'] = prompt_scores\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_directory(self, directory):\n",
    "        \"\"\"Evaluate all images in directory.\"\"\"\n",
    "        print(f\"\\nEvaluating images in {directory}\")\n",
    "        \n",
    "        results = []\n",
    "        image_files = [f for f in os.listdir(directory) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        for filename in tqdm(image_files, desc=\"Evaluating images\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            result = self.evaluate_image(image_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze evaluation results.\"\"\"\n",
    "        analysis = {\n",
    "            'overall_scores': {},\n",
    "            'category_scores': {},\n",
    "            'prompt_scores': {},\n",
    "            'per_image_analysis': []\n",
    "        }\n",
    "        \n",
    "        for result in results:\n",
    "            image_scores = {}\n",
    "            \n",
    "            # Calculate average scores per category\n",
    "            for category, scores in result['scores'].items():\n",
    "                if category != 'prompts':\n",
    "                    category_avg = np.mean(list(scores.values()))\n",
    "                    image_scores[f'{category}_avg'] = category_avg\n",
    "                    \n",
    "                    if category not in analysis['category_scores']:\n",
    "                        analysis['category_scores'][category] = []\n",
    "                    analysis['category_scores'][category].append(category_avg)\n",
    "            \n",
    "            # Calculate prompt score\n",
    "            if 'prompts' in result['scores']:\n",
    "                prompt_avg = np.mean(list(result['scores']['prompts'].values()))\n",
    "                image_scores['prompt_avg'] = prompt_avg\n",
    "                analysis['prompt_scores'][result['filename']] = prompt_avg\n",
    "            \n",
    "            # Add to per-image analysis\n",
    "            image_scores['filename'] = result['filename']\n",
    "            image_scores['overall_score'] = np.mean(list(image_scores.values())[:-1])  # Exclude filename\n",
    "            analysis['per_image_analysis'].append(image_scores)\n",
    "        \n",
    "        # Calculate overall averages\n",
    "        analysis['overall_scores'] = {\n",
    "            'overall_average': np.mean([img['overall_score'] \n",
    "                                      for img in analysis['per_image_analysis']]),\n",
    "            'category_averages': {\n",
    "                category: np.mean(scores)\n",
    "                for category, scores in analysis['category_scores'].items()\n",
    "            },\n",
    "            'prompt_average': np.mean(list(analysis['prompt_scores'].values()))\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def generate_report(self, analysis, output_dir='evaluation_results'):\n",
    "        \"\"\"Generate comprehensive evaluation report.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save detailed results\n",
    "        df = pd.DataFrame(analysis['per_image_analysis'])\n",
    "        df.to_csv(os.path.join(output_dir, 'detailed_scores.csv'), index=False)\n",
    "        \n",
    "        # Generate report text\n",
    "        report = [\n",
    "            \"=== MBZUAI Paper Cup Generation Evaluation ===\\n\",\n",
    "            \"Overall Scores:\",\n",
    "            f\"Average Score: {analysis['overall_scores']['overall_average']:.4f}\",\n",
    "            f\"Average Prompt Score: {analysis['overall_scores']['prompt_average']:.4f}\\n\",\n",
    "            \"Category Averages:\"\n",
    "        ]\n",
    "        \n",
    "        for category, score in analysis['overall_scores']['category_averages'].items():\n",
    "            report.append(f\"{category}: {score:.4f}\")\n",
    "        \n",
    "        # Add top performing images\n",
    "        df_sorted = pd.DataFrame(analysis['per_image_analysis']).sort_values(\n",
    "            'overall_score', ascending=False)\n",
    "        \n",
    "        report.extend([\n",
    "            \"\\nTop 3 Performing Images:\"\n",
    "        ])\n",
    "        \n",
    "        for _, row in df_sorted.head(3).iterrows():\n",
    "            report.append(f\"{row['filename']}: {row['overall_score']:.4f}\")\n",
    "        \n",
    "        # Save report\n",
    "        with open(os.path.join(output_dir, 'evaluation_report.txt'), 'w') as f:\n",
    "            f.write('\\n'.join(report))\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.plot_results(analysis, output_dir)\n",
    "\n",
    "    def plot_results(self, analysis, output_dir):\n",
    "        \"\"\"Create visualizations of results.\"\"\"\n",
    "        # Plot 1: Category Performance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        categories = list(analysis['overall_scores']['category_averages'].keys())\n",
    "        scores = list(analysis['overall_scores']['category_averages'].values())\n",
    "        \n",
    "        plt.bar(categories, scores)\n",
    "        plt.title('Average Performance by Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'category_performance.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot 2: Individual Image Performance\n",
    "        df = pd.DataFrame(analysis['per_image_analysis'])\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        sns.barplot(data=df, x='filename', y='overall_score')\n",
    "        plt.title('Overall Score by Image')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'image_scores.png'))\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    generated_dir = \"output\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = MBZUAICLIPEvaluator()\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluator.evaluate_directory(generated_dir)\n",
    "        \n",
    "        # Analyze results\n",
    "        analysis = evaluator.analyze_results(results)\n",
    "        \n",
    "        # Generate report\n",
    "        evaluator.generate_report(analysis)\n",
    "        \n",
    "        print(\"\\nEvaluation completed successfully!\")\n",
    "        print(\"Results saved in evaluation_results/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CLIP model...\n",
      "\n",
      "Evaluating images in output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images:  22%|██▏       | 2/9 [00:00<00:00, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores for generated_image_8.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3175\n",
      "Logo Quality   : 0.2708\n",
      "Cup Quality    : 0.2948\n",
      "Overall Image  : 0.2987\n",
      "--------------------------------------------------\n",
      "\n",
      "Scores for generated_image_10.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3243\n",
      "Logo Quality   : 0.2801\n",
      "Cup Quality    : 0.3005\n",
      "Overall Image  : 0.3077\n",
      "--------------------------------------------------\n",
      "\n",
      "Scores for generated_image_6.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3371\n",
      "Logo Quality   : 0.2969\n",
      "Cup Quality    : 0.3148\n",
      "Overall Image  : 0.3245\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images:  44%|████▍     | 4/9 [00:00<00:00, 13.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores for generated_image_7.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3189\n",
      "Logo Quality   : 0.2856\n",
      "Cup Quality    : 0.2930\n",
      "Overall Image  : 0.3167\n",
      "--------------------------------------------------\n",
      "\n",
      "Scores for generated_image_9.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3354\n",
      "Logo Quality   : 0.2929\n",
      "Cup Quality    : 0.3087\n",
      "Overall Image  : 0.3053\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images:  67%|██████▋   | 6/9 [00:00<00:00, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores for generated_image_3.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.2845\n",
      "Logo Quality   : 0.2744\n",
      "Cup Quality    : 0.2787\n",
      "Overall Image  : 0.2720\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images:  89%|████████▉ | 8/9 [00:00<00:00, 14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores for generated_image_4.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3129\n",
      "Logo Quality   : 0.2794\n",
      "Cup Quality    : 0.2955\n",
      "Overall Image  : 0.2954\n",
      "--------------------------------------------------\n",
      "\n",
      "Scores for generated_image_5.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3397\n",
      "Logo Quality   : 0.3096\n",
      "Cup Quality    : 0.3151\n",
      "Overall Image  : 0.3236\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating images: 100%|██████████| 9/9 [00:00<00:00, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores for generated_image_2.png:\n",
      "--------------------------------------------------\n",
      "Main Prompt    : 0.3306\n",
      "Logo Quality   : 0.2813\n",
      "Cup Quality    : 0.3040\n",
      "Overall Image  : 0.3149\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Summary Statistics ===\n",
      "\n",
      "Main Prompt:\n",
      "  Average: 0.3223\n",
      "  Std Dev: 0.0160\n",
      "\n",
      "Logo Quality:\n",
      "  Average: 0.2857\n",
      "  Std Dev: 0.0115\n",
      "\n",
      "Cup Quality:\n",
      "  Average: 0.3006\n",
      "  Std Dev: 0.0110\n",
      "\n",
      "Overall Image:\n",
      "  Average: 0.3065\n",
      "  Std Dev: 0.0155\n",
      "\n",
      "Best performing image: generated_image_5.png (Score: 0.3220)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# direct_clip_evaluator.py\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DirectCLIPEvaluator:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load CLIP model and processor\n",
    "        print(\"Loading CLIP model...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Define evaluation aspects\n",
    "        self.evaluation_aspects = {\n",
    "            \"Main Prompt\": \"a white paper cup with MBZUAI logo\",\n",
    "            \"Logo Quality\": \"clear visible MBZUAI logo on a cup\",\n",
    "            \"Cup Quality\": \"high quality white paper cup\",\n",
    "            \"Overall Image\": \"professional product photography of a paper cup\"\n",
    "        }\n",
    "\n",
    "    def calculate_similarity(self, image, text):\n",
    "        \"\"\"Calculate CLIP similarity score between image and text.\"\"\"\n",
    "        try:\n",
    "            # Process image and text\n",
    "            image_inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            text_inputs = self.clip_processor(text=[text], return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            \n",
    "            # Get features\n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.get_image_features(**image_inputs)\n",
    "                text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = torch.cosine_similarity(text_features, image_features).item()\n",
    "            \n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_image(self, image_path):\n",
    "        \"\"\"Evaluate a single image against all aspects.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            scores = {}\n",
    "            \n",
    "            for aspect, prompt in self.evaluation_aspects.items():\n",
    "                score = self.calculate_similarity(image, prompt)\n",
    "                if score is not None:\n",
    "                    scores[aspect] = score\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def print_scores(self, filename, scores):\n",
    "        \"\"\"Print scores in a formatted way.\"\"\"\n",
    "        print(f\"\\nScores for {filename}:\")\n",
    "        print(\"-\" * 50)\n",
    "        for aspect, score in scores.items():\n",
    "            print(f\"{aspect:15}: {score:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    def evaluate_directory(self, directory):\n",
    "        \"\"\"Evaluate all images in directory and show results.\"\"\"\n",
    "        print(f\"\\nEvaluating images in {directory}\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_files = [f for f in os.listdir(directory) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        all_scores = []\n",
    "        \n",
    "        # Evaluate each image\n",
    "        for filename in tqdm(image_files, desc=\"Evaluating images\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            scores = self.evaluate_image(image_path)\n",
    "            \n",
    "            if scores:\n",
    "                all_scores.append({\n",
    "                    'filename': filename,\n",
    "                    'scores': scores\n",
    "                })\n",
    "                self.print_scores(filename, scores)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        if all_scores:\n",
    "            print(\"\\n=== Summary Statistics ===\")\n",
    "            \n",
    "            # Calculate averages for each aspect\n",
    "            aspects = self.evaluation_aspects.keys()\n",
    "            for aspect in aspects:\n",
    "                scores = [item['scores'][aspect] for item in all_scores \n",
    "                         if aspect in item['scores']]\n",
    "                if scores:\n",
    "                    avg_score = np.mean(scores)\n",
    "                    std_score = np.std(scores)\n",
    "                    print(f\"\\n{aspect}:\")\n",
    "                    print(f\"  Average: {avg_score:.4f}\")\n",
    "                    print(f\"  Std Dev: {std_score:.4f}\")\n",
    "            \n",
    "            # Find best performing image\n",
    "            avg_scores = [(item['filename'], np.mean(list(item['scores'].values())))\n",
    "                         for item in all_scores]\n",
    "            best_img = max(avg_scores, key=lambda x: x[1])\n",
    "            \n",
    "            print(f\"\\nBest performing image: {best_img[0]} (Score: {best_img[1]:.4f})\")\n",
    "\n",
    "def main():\n",
    "    # Directory containing generated images\n",
    "    generated_dir = \"output\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize evaluator\n",
    "        evaluator = DirectCLIPEvaluator()\n",
    "        \n",
    "        # Run evaluation\n",
    "        evaluator.evaluate_directory(generated_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading ResNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishita.agarwal/.conda/envs/stable/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ishita.agarwal/.conda/envs/stable/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "\n",
      "Evaluating images in output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 9/9 [00:02<00:00,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Concept Accuracy Scores ===\n",
      "Number of images evaluated: 9\n",
      "\n",
      "Category Averages:\n",
      "cup       : 0.2831\n",
      "logo      : 0.2155\n",
      "quality   : 0.1938\n",
      "\n",
      "Overall Concept Accuracy: 0.2308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# combined_concept_accuracy.py\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ConceptAccuracyEvaluator:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize ResNet\n",
    "        print(\"Loading ResNet...\")\n",
    "        self.resnet = models.resnet50(pretrained=True).to(self.device)\n",
    "        self.resnet.eval()\n",
    "        \n",
    "        # Initialize CLIP\n",
    "        print(\"Loading CLIP...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Preprocessing for ResNet\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        # Load ImageNet classes\n",
    "        try:\n",
    "            with open('imagenet_class_index.json') as f:\n",
    "                self.imagenet_classes = {int(key): value[1] \n",
    "                                       for key, value in json.load(f).items()}\n",
    "        except:\n",
    "            print(\"Warning: ImageNet class index file not found.\")\n",
    "            self.imagenet_classes = {}\n",
    "        \n",
    "        # Define concepts to check\n",
    "        self.concepts = {\n",
    "            \"cup\": [\"cup\", \"paper cup\", \"coffee cup\", \"disposable cup\"],\n",
    "            \"logo\": [\"logo\", \"brand logo\", \"company logo\", \"MBZUAI logo\"],\n",
    "            \"quality\": [\"high quality\", \"clear image\", \"professional photo\"]\n",
    "        }\n",
    "\n",
    "    def check_clip_concept(self, image, concept):\n",
    "        \"\"\"Check concept using CLIP.\"\"\"\n",
    "        try:\n",
    "            image_inputs = self.clip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            text_inputs = self.clip_processor(text=[concept], return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.get_image_features(**image_inputs)\n",
    "                text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "                \n",
    "                image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "                \n",
    "                similarity = torch.cosine_similarity(image_features, text_features).item()\n",
    "            \n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in CLIP check: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def check_resnet_concept(self, image):\n",
    "        \"\"\"Check concept using ResNet.\"\"\"\n",
    "        try:\n",
    "            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.resnet(image_tensor)\n",
    "                _, predicted_idx = outputs.max(1)\n",
    "                \n",
    "                if self.imagenet_classes:\n",
    "                    predicted_label = self.imagenet_classes[predicted_idx.item()]\n",
    "                    return predicted_label.lower()\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ResNet check: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def evaluate_directory(self, directory):\n",
    "        \"\"\"Evaluate all images in directory.\"\"\"\n",
    "        print(f\"\\nEvaluating images in {directory}\")\n",
    "        \n",
    "        image_files = [f for f in os.listdir(directory) \n",
    "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            print(\"No images found in directory!\")\n",
    "            return\n",
    "        \n",
    "        results = {concept: [] for concept in self.concepts.keys()}\n",
    "        \n",
    "        # Process each image\n",
    "        for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "            try:\n",
    "                image_path = os.path.join(directory, filename)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # ResNet check\n",
    "                resnet_label = self.check_resnet_concept(image)\n",
    "                \n",
    "                # CLIP check for each concept category\n",
    "                for category, concept_list in self.concepts.items():\n",
    "                    category_scores = []\n",
    "                    for concept in concept_list:\n",
    "                        clip_score = self.check_clip_concept(image, concept)\n",
    "                        category_scores.append(clip_score)\n",
    "                    \n",
    "                    # Combine ResNet and CLIP results\n",
    "                    if any(concept in resnet_label for concept in concept_list):\n",
    "                        category_scores.append(1.0)\n",
    "                        \n",
    "                    results[category].append(np.mean(category_scores))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "        \n",
    "        # Calculate and print average scores\n",
    "        print(\"\\n=== Concept Accuracy Scores ===\")\n",
    "        print(f\"Number of images evaluated: {len(image_files)}\")\n",
    "        print(\"\\nCategory Averages:\")\n",
    "        \n",
    "        overall_scores = []\n",
    "        for category, scores in results.items():\n",
    "            avg_score = np.mean(scores)\n",
    "            overall_scores.append(avg_score)\n",
    "            print(f\"{category:10}: {avg_score:.4f}\")\n",
    "        \n",
    "        print(f\"\\nOverall Concept Accuracy: {np.mean(overall_scores):.4f}\")\n",
    "\n",
    "def main():\n",
    "    generated_dir = \"output\"\n",
    "    \n",
    "    try:\n",
    "        evaluator = ConceptAccuracyEvaluator()\n",
    "        evaluator.evaluate_directory(generated_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
